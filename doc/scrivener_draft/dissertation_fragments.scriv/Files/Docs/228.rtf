{\rtf1\ansi\ansicpg1252\cocoartf1187\cocoasubrtf400
{\fonttbl\f0\fnil\fcharset0 Cochin;}
{\colortbl;\red255\green255\blue255;}
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\fi360\sl288\slmult1\pardirnatural

\f0\fs28 \cf0 \
##Problem [maps:problem]\
\
\
Now that we have established how to sense the environment, how the robot will be controlled, and how we represent the position and orientation of the robot space, we would now like to synthesize these components into an overall map.  We can formalize the definition of the mapping problem as follows.\
\
Given the posture images \\\\(I_\{1:t\}\\\\) and the actions \\\\(A_\{1:t\}\\\\), solve for each \\\\(X_\{1:t\}\\\\) as shown in [](#maps_inputs).  At each anchored position \\\\(X_k\\\\) between locomotion actions \\\\(A_k\\\\), we have a posture image \\\\(I_k\\\\) for both the front and back probe sweeps respectively.  These posture images created while the robot is traveling through the environment need a map representation suited to the robot's experience.\
\
![Mapping inputs][maps_inputs]\
\
[maps_inputs]: {\field{\*\fldinst{HYPERLINK "scrivlnk://286"}}{\fldrslt maps_inputs}} width=400px\
\
We define the actions of the robot to be \\\\(A_\{1:t\} = \{f, b, f, f, f, \'85 , b, b\}\\\\) where:\
<!--\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\fi360\sl288\slmult1\pardirnatural
\cf0 \expnd0\expndtw0\kerning0
\\[\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
    A_i = \
\\begin\{cases\}\
    f, & \\text\{forward locomotion step\}\\\\\
    b, & \\text\{backward locomotion step\}\\\\\
    \\varnothing, & \\text\{no move (switch sweep side)\}\
\\end\{cases\}\
\\]\
-->\kerning1\expnd0\expndtw0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\fi360\sl288\slmult1\pardirnatural
\cf0 \
The pose \\\\(X_i = (x_i, y_i, \\theta_i) \\\\), is the position and orientation of the robot's posture frame \\\\(P\\\\) with respect to the global frame \\\\(G\\\\).  The posture frame is computed from the posture curve, \\\\(\\beta_i\\\\), described in [](#chap:position).\
\
Each posture image \\\\(I_i\\\\) represents the swept void space of the environment, described in [](#chap:sensing).  Each \\\\(I_i\\\\) may have some spatial landmarks, described in [](#chap:landmarks).\
\
![Posture Images to Spatial Curves][image_to_spatial_curve]\
\
[image_to_spatial_curve]: {\field{\*\fldinst{HYPERLINK "scrivlnk://287"}}{\fldrslt image_to_spatial_curve}} width=400px\
\
For our mapping method, we often use the *spatial curve* \\\\(C_k\\\\), derived from the *posture image* \\\\(I_k\\\\).  Though we don't explicitly show it, the computation of \\\\(C_k = \\mathrm\{scurve\}(I_k)\\\\), shown in [](#image_to_spatial_curve), is implied throughout the rest of this dissertation.\
\
In order to build a map of the environment, we require sensor features that can be used to build geometric relationships between observations of the environment at different poses.  In many existing SLAM algorithms, an abundance of landmark features can be used to build correspondence between different observations.\
\
In our approach, we only have a sparse amount of landmarks corresponding to spatial features which indicate the presence of junctions.  There are no landmarks at all for the vast majority of the pipe-like environments.  Existing feature-based SLAM algorithms cannot operate on such sparse data.  Furthermore, the range of sensing of the robot is very limited, and does not give a very large view of the environment.  This causes many locations in the environment to look identical.  We need a mapping approach that can map with such limited data, using only spatial curves and spatial features.\
}